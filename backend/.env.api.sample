# =============================================================================
# DocuChat Backend - Environment Variables Sample (API Mode)
# =============================================================================
# This configuration uses external LLM APIs for chat/reasoning while
# keeping Ollama for embeddings only.
#
# Usage:
#   1. Copy this file to .env
#   2. Configure your API keys
#   3. Run: docker compose -f docker-compose-api.yml up -d
# =============================================================================

# -----------------------------------------------------------------------------
# Django Settings
# -----------------------------------------------------------------------------
DEBUG=True
SECRET_KEY=change-me-in-production
ALLOWED_HOSTS=localhost,127.0.0.1,backend,nginx

# -----------------------------------------------------------------------------
# Storage Paths (must match docker-compose volume mounts)
# -----------------------------------------------------------------------------
UPLOAD_ROOT=/data/uploads
EXTRACTED_ROOT=/data/extracted

# -----------------------------------------------------------------------------
# Database (PostgreSQL)
# -----------------------------------------------------------------------------
DATABASE_URL=postgres://docuchat:change_me@postgres:5432/docuchat

# -----------------------------------------------------------------------------
# Redis / Celery
# -----------------------------------------------------------------------------
REDIS_URL=redis://redis:6379/0
CELERY_BROKER_URL=redis://redis:6379/0

# -----------------------------------------------------------------------------
# Keycloak / JWT Auth
# -----------------------------------------------------------------------------
KC_BASE_URL=http://keycloak:8080
KC_REALM=docuchat
KC_AUDIENCE=docuchat-frontend
KC_EXTERNAL_ISSUER=http://localhost/realms/docuchat

# -----------------------------------------------------------------------------
# LLM Provider Configuration
# -----------------------------------------------------------------------------
# Choose which LLM provider to use for chat/reasoning:
#   - "ollama"  : Local Ollama inference (default)
#   - "gemini"  : Google Gemini API
#   - "openai"  : OpenAI or compatible APIs (Azure, Groq, Together, etc.)
# 
# Note: Embeddings ALWAYS use Ollama (nomic-embed-text) regardless of this setting
# -----------------------------------------------------------------------------
LLM_PROVIDER=gemini

# -----------------------------------------------------------------------------
# Ollama Configuration (used for embeddings, and LLM if LLM_PROVIDER=ollama)
# -----------------------------------------------------------------------------
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_EMBED_MODEL=nomic-embed-text
# OLLAMA_CHAT_MODEL is not used when LLM_PROVIDER is not "ollama"
# OLLAMA_CHAT_MODEL=gemma:7b

# -----------------------------------------------------------------------------
# Google Gemini API Configuration (used when LLM_PROVIDER=gemini)
# -----------------------------------------------------------------------------
# Get your API key from: https://aistudio.google.com/apikey
# 
# Available models:
#   - gemini-1.5-flash   : Fast, efficient, good for most tasks (recommended)
#   - gemini-1.5-pro     : More capable, better for complex reasoning
#   - gemini-2.0-flash   : Latest model, very fast
# -----------------------------------------------------------------------------
GEMINI_API_KEY=api-key-here
GEMINI_MODEL=model-name-here
GEMINI_TIMEOUT=120

# -----------------------------------------------------------------------------
# OpenAI-Compatible API Configuration (used when LLM_PROVIDER=openai)
# -----------------------------------------------------------------------------
# Works with: OpenAI, Azure OpenAI, Groq, Together AI, Fireworks, etc.
#
# For OpenAI:
#   OPENAI_API_KEY=sk-...
#   OPENAI_BASE_URL=https://api.openai.com/v1
#   OPENAI_MODEL=gpt-4o-mini
#
# For Azure OpenAI:
#   OPENAI_API_KEY=your-azure-key
#   OPENAI_BASE_URL=https://your-resource.openai.azure.com/openai/deployments/your-deployment
#   OPENAI_MODEL=gpt-4o-mini
#
# For Groq:
#   OPENAI_API_KEY=gsk_...
#   OPENAI_BASE_URL=https://api.groq.com/openai/v1
#   OPENAI_MODEL=llama-3.3-70b-versatile
#
# For Together AI:
#   OPENAI_API_KEY=your-together-key
#   OPENAI_BASE_URL=https://api.together.xyz/v1
#   OPENAI_MODEL=meta-llama/Llama-3.3-70B-Instruct-Turbo
# -----------------------------------------------------------------------------
# OPENAI_API_KEY=your-openai-api-key-here
# OPENAI_BASE_URL=https://api.openai.com/v1
# OPENAI_MODEL=gpt-4o-mini
# OPENAI_TIMEOUT=120

# -----------------------------------------------------------------------------
# Feature Flags
# -----------------------------------------------------------------------------
ENABLE_QUERY_REFINEMENT=True
ENABLE_RERANKER=True
